{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPadZCB4fMKGvRx7HobrbkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardoskewes/68610finalprojectgroup48/blob/colab_branch/truly_final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8lv6t4dizPQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "rm -rf 68610finalprojectgroup48\n",
        "git clone https://github.com/ricardoskewes/68610finalprojectgroup48.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38VHNhKhgtjq",
        "outputId": "6939aff7-5fb2-405a-af65-d7ed0dfacc8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into '68610finalprojectgroup48'...\n",
            "Updating files:  79% (4002/5007)\rUpdating files:  80% (4006/5007)\rUpdating files:  81% (4056/5007)\rUpdating files:  82% (4106/5007)\rUpdating files:  83% (4156/5007)\rUpdating files:  84% (4206/5007)\rUpdating files:  85% (4256/5007)\rUpdating files:  86% (4307/5007)\rUpdating files:  87% (4357/5007)\rUpdating files:  88% (4407/5007)\rUpdating files:  89% (4457/5007)\rUpdating files:  90% (4507/5007)\rUpdating files:  91% (4557/5007)\rUpdating files:  92% (4607/5007)\rUpdating files:  93% (4657/5007)\rUpdating files:  94% (4707/5007)\rUpdating files:  95% (4757/5007)\rUpdating files:  96% (4807/5007)\rUpdating files:  97% (4857/5007)\rUpdating files:  98% (4907/5007)\rUpdating files:  99% (4957/5007)\rUpdating files: 100% (5007/5007)\rUpdating files: 100% (5007/5007), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu simpletransformers\n",
        "!pip install wordfreq\n",
        "!pip install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sxSH2PXi75h",
        "outputId": "95442e0c-5b04-4e25-ce39-2f1f81af063a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simpletransformers\n",
            "  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.66.6)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.46.3)\n",
            "Collecting datasets (from simpletransformers)\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.2)\n",
            "Collecting seqeval (from simpletransformers)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.17.1)\n",
            "Collecting tensorboardx (from simpletransformers)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.2.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.20.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.18.7)\n",
            "Collecting streamlit (from simpletransformers)\n",
            "  Downloading streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2024.8.30)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->simpletransformers)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->simpletransformers)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->simpletransformers)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.11.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (11.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit->simpletransformers)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.1.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=3ec85914fe3370b220aacf401c9d7154f39afce0a3e7e7068d54f1ae8188eb6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: xxhash, watchdog, tensorboardx, portalocker, fsspec, dill, colorama, sacrebleu, pydeck, multiprocess, seqeval, streamlit, datasets, simpletransformers\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 portalocker-3.0.0 pydeck-0.9.1 sacrebleu-2.4.3 seqeval-1.2.2 simpletransformers-0.70.1 streamlit-1.40.2 tensorboardx-2.6.2.2 watchdog-6.0.0 xxhash-3.5.0\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ftfy>=6.1 (from wordfreq)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (3.5.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (1.1.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from wordfreq) (2024.9.11)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.1.0)\n",
            "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: locate, ftfy, wordfreq\n",
            "Successfully installed ftfy-6.3.1 locate-1.1.1 wordfreq-3.1.1\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and imports"
      ],
      "metadata": {
        "id": "FY-dNU8wgxl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Append the cloned repository to sys.path\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/68610finalprojectgroup48\")\n",
        "projectDir = \"/content/68610finalprojectgroup48/tweepfake\"\n",
        "sys.path.insert(0, projectDir)\n",
        "resultsDir = projectDir+\"/data/results\"\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "# Data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# NLP libraries\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from wordfreq import word_frequency\n",
        "\n",
        "#  confusion matrix stuff\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning libraries\n",
        "import torch\n",
        "from transformers import set_seed\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "# wandb library\n",
        "import wandb\n",
        "\n",
        "#DataHandler\n",
        "from DataHandler import DataHandler\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_state = 523\n",
        "set_seed(random_state)\n"
      ],
      "metadata": {
        "id": "xtsJvTficTl4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "swRXxgAuhgdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading code (unchanged)\n",
        "from DataHandler import DataHandler\n",
        "dh = DataHandler()\n",
        "\n",
        "# Paths to datasets\n",
        "csvTrainDataset = projectDir + \"/data/splits/train.csv\"\n",
        "csvValDataset = projectDir + \"/data/splits/validation.csv\"\n",
        "csvTestDataset = projectDir + \"/data/splits/test.csv\"\n",
        "\n",
        "# Load datasets\n",
        "dfTrain = dh.readCSVData(csvTrainDataset)\n",
        "dfVal = dh.readCSVData(csvValDataset)\n",
        "dfTest = dh.readCSVData(csvTestDataset)\n",
        "\n",
        "# Select interesting columns for this study\n",
        "dfTrainDataset = dfTrain[[\"screen_name\", \"text\", \"account.type\"]]\n",
        "dfValDataset = dfVal[[\"screen_name\", \"text\", \"account.type\"]]\n",
        "dfTestDataset = dfTest[[\"screen_name\", \"text\", \"account.type\"]]\n",
        "\n",
        "# Prepare training data\n",
        "X_train_all = dfTrainDataset.drop(columns=['screen_name'])\n",
        "X_train_all.columns = [\"text\", \"label\"]\n",
        "\n",
        "X_val_all = dfValDataset.drop(columns=['screen_name'])\n",
        "X_val_all.columns = [\"text\", \"label\"]\n",
        "\n",
        "X_test_all = dfTestDataset.drop(columns=['screen_name'])\n",
        "X_test_all.columns = [\"text\", \"label\"]\n",
        "\n",
        "# Map labels to integers\n",
        "dictLabels = {\"human\": 0, \"bot\": 1}\n",
        "\n",
        "X_train_all[\"label\"] = X_train_all[\"label\"].apply(lambda x: dictLabels[x])\n",
        "X_val_all[\"label\"] = X_val_all[\"label\"].apply(lambda x: dictLabels[x])\n",
        "X_test_all[\"label\"] = X_test_all[\"label\"].apply(lambda x: dictLabels[x])\n",
        "\n",
        "# Extract labels\n",
        "y_train = X_train_all[\"label\"]\n",
        "y_val = X_val_all[\"label\"]\n",
        "y_test = X_test_all[\"label\"]\n",
        "\n",
        "train_labels = y_train.tolist()\n",
        "val_labels = y_val.tolist()\n",
        "test_labels = y_test.tolist()\n"
      ],
      "metadata": {
        "id": "J_jOztOzcS3K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Functions"
      ],
      "metadata": {
        "id": "SEQ7bb-jhzzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature creating functions (unchanged)\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_spacy_doc(text):\n",
        "    return spacy_nlp(text)\n",
        "\n",
        "def switch_spacy_to_text(func):\n",
        "    def inner1(*args, **kwargs):\n",
        "        spacy_doc = get_spacy_doc(args[0])\n",
        "        return func(spacy_doc, **kwargs)\n",
        "    return inner1\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())  # C1\n",
        "\n",
        "def avg_word_length(text):\n",
        "    words = text.split()  # C2\n",
        "    return np.mean([len(w) for w in words])\n",
        "\n",
        "@switch_spacy_to_text\n",
        "def get_ANP(spacy_doc):\n",
        "    # Get total length\n",
        "    total_tokens = len([token for token in spacy_doc if not token.is_punct and not token.is_space])\n",
        "    # List of adjectives, nouns, and pronouns ('ANP')\n",
        "    adjectives = [token.text for token in spacy_doc if token.pos_ == \"ADJ\"]\n",
        "    nouns = [token.text for token in spacy_doc if token.pos_ == \"NOUN\"]\n",
        "    pronouns = [token.text for token in spacy_doc if token.pos_ == \"PRON\"]\n",
        "    # Estimate densities\n",
        "    adj_density = len(adjectives) / total_tokens\n",
        "    noun_density = len(nouns) / total_tokens\n",
        "    pronoun_density = len(pronouns) / total_tokens\n",
        "    return adjectives, nouns, pronouns, adj_density, noun_density, pronoun_density\n",
        "\n",
        "def get_ANP_clean(text):\n",
        "    _, _, _, adj_density, noun_density, pronoun_density = get_ANP(text)\n",
        "    return adj_density, noun_density, pronoun_density  # C3\n",
        "\n",
        "@switch_spacy_to_text\n",
        "def get_capitalizations(spacy_doc):\n",
        "    capitalizations = [token.text for token in spacy_doc if token.text.isupper()]\n",
        "    return capitalizations, len(capitalizations) / len(spacy_doc)\n",
        "\n",
        "def get_capitalizations_clean(text):\n",
        "    _, cap_ratio = get_capitalizations(text)\n",
        "    return np.round(cap_ratio, 2)  # C4\n",
        "\n",
        "@switch_spacy_to_text\n",
        "def get_sentiment(spacy_doc):\n",
        "    polarity, subjectivity = TextBlob(spacy_doc.text).sentiment\n",
        "    return np.round(polarity, 3), np.round(subjectivity, 3)  # C5\n",
        "\n",
        "@switch_spacy_to_text\n",
        "def calculate_rarity_scores(spacy_doc, lang='en'):\n",
        "    adj_rarity_scores = []\n",
        "    noun_rarity_scores = []\n",
        "\n",
        "    for token in spacy_doc:\n",
        "        if token.pos_ == \"ADJ\":\n",
        "            adjective = token.text.lower()\n",
        "            freq = word_frequency(adjective, lang)\n",
        "            adj_rarity_score = -math.log(freq) if freq > 0 else 0\n",
        "            adj_rarity_scores.append(adj_rarity_score)\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            noun = token.text.lower()\n",
        "            freq = word_frequency(noun, lang)\n",
        "            noun_rarity_score = -math.log(freq) if freq > 0 else 0\n",
        "            noun_rarity_scores.append(noun_rarity_score)\n",
        "    res = [0, 0]\n",
        "    # Calculate median rarity score over all NAs\n",
        "    if noun_rarity_scores:\n",
        "        res[0] = np.median(noun_rarity_scores)\n",
        "    if adj_rarity_scores:\n",
        "        res[1] = np.median(adj_rarity_scores)\n",
        "    return tuple(np.round(res, 3))\n",
        "\n",
        "@switch_spacy_to_text\n",
        "def get_punctuation(spacy_doc):\n",
        "    punctuation = [token.text for token in spacy_doc if token.pos_ == \"PUNCT\"]\n",
        "    return punctuation, len(punctuation) / len(spacy_doc)\n",
        "\n",
        "def get_punctuation_clean(text):\n",
        "    _, punct_ratio = get_punctuation(text)\n",
        "    return np.round(punct_ratio, 2)  # C7\n",
        "\n",
        "# Dictionary of classical features\n",
        "C = {\n",
        "    1: count_words,             # C1: Number of words in sentence\n",
        "    2: avg_word_length,         # C2: Average word length in sentence\n",
        "    3: get_ANP_clean,           # C3: Density scores of adjectives, pronouns, and nouns\n",
        "    4: get_capitalizations_clean,  # C4: Density of capital letters\n",
        "    5: get_sentiment,           # C5: Sentiment analysis\n",
        "    6: calculate_rarity_scores, # C6: Noun and adjective rarity scores\n",
        "    7: get_punctuation_clean    # C7: Density of punctuation\n",
        "}\n"
      ],
      "metadata": {
        "id": "-VsA8zDPhk39"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definitions"
      ],
      "metadata": {
        "id": "z6sszpdYjJxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model descriptions\n",
        "model_descriptions = {\n",
        "    1: 'BERT*(tweet)',\n",
        "    2: 'BERT*({tweet <SEP> C1 <SEP> C2 ...})',\n",
        "    3: 'BERT*({tweet <FEAT> C1 <FEAT> C2 ...})',\n",
        "}\n",
        "\n",
        "# Model-specific arguments (excluding hyperparameters). THESE ARE NOT MEANT TO BE SWEPT OVER, BUT JUST DEFINED.\n",
        "model_args_dict = {\n",
        "    1: {'transformer_type': 'bert', 'transformer_name': 'bert-base-cased', 'output-dir': 'type1'},\n",
        "    2: {'transformer_type': 'bert', 'transformer_name': 'bert-base-cased', 'output-dir': 'type2'},\n",
        "    3: {'separator_token_name': 'FEAT', 'transformer_type': 'bert', 'transformer_name': 'bert-base-cased', 'output-dir': 'type3'},  # Additional arguments can be added here\n",
        "    # Add arguments for models 4 to 9\n",
        "}\n",
        "\n",
        "# Define which hyperparameters are applicable to each model. THESE ARE MEANT TO BE ABLE TO BE SWEPT OVER. OVERRIDE DEFAULTS USING WANDB SWEEPS\n",
        "model_hyperparameters = {\n",
        "    1: {\"learning_rate\": 1e-5, \"train_batch_size\": 16, \"num_train_epochs\": 3, \"max_seq_length\": 128},\n",
        "    2: {\"learning_rate\": 1e-5, \"train_batch_size\": 16, \"num_train_epochs\": 3, \"max_seq_length\": 128, \"C_ids\": [1,2]},\n",
        "    3: {\"learning_rate\": 1e-5, \"train_batch_size\": 16, \"num_train_epochs\": 3, \"max_seq_length\": 128, \"C_ids\": [3,4,5]},\n",
        "}"
      ],
      "metadata": {
        "id": "cGVUaliQjIlK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Initialize wandb\n"
      ],
      "metadata": {
        "id": "-lxrQxwyjhd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# Set the wandb project name\n",
        "WANDB_PROJECT_NAME = \"6-861-finalproj\"\n"
      ],
      "metadata": {
        "id": "qxJvmgEZjbba"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "YZxhABzPjdVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# all this thing outputs is the dictionary needed to append special tokens\n",
        "def create_special_tokens_dict(model_id, model_args_dict):\n",
        "    \"\"\"\n",
        "    Create a dictionary of special tokens based on the model_id.\n",
        "\n",
        "    Args:\n",
        "        model_id (int): The ID of the model to determine special tokens.\n",
        "        model_args_dict (dict): Dictionary of model-specific arguments.\n",
        "        hyperparameters (dict): Dictionary of hyperparameters for training.\n",
        "    Returns:\n",
        "        dict: Dictionary of special tokens.\n",
        "    \"\"\"\n",
        "    additional_tokens = []\n",
        "    if model_id == 3:\n",
        "        additional_tokens.append(model_args_dict['separator_token_name'])\n",
        "    return {'additional_special_tokens': additional_tokens}\n",
        "\n",
        "\n",
        "\n",
        "# This thing takes in a\n",
        "def initialize_model_with_special_tokens (model_id, model_args_dict, hyperparameters, special_tokens_dict, model_descriptions):\n",
        "    \"\"\"\n",
        "    Initialize the model and tokenizer with special tokens.\n",
        "\n",
        "    Parameters:\n",
        "        transformer_type (str): The type of the transformer (e.g., 'bert').\n",
        "        transformer_name (str): The name of the pretrained transformer model.\n",
        "        model_id (int): The ID of the model.\n",
        "        hyperparameters (dict): Dictionary of hyperparameters, both for training (i.e., learning_rate, max_seq_length, etc.) and otherwise fine-tunable (for some model_ids an example is numbeer of classical features).\n",
        "        model_args_dict: (dict): Dictionary of model-specific arguments.\n",
        "        special_tokens_dict (dict): Dictionary of special tokens to add.\n",
        "\n",
        "    Returns:\n",
        "        model: The initialized model.\n",
        "        tokenizer: The tokenizer with added special tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize model arguments\n",
        "    model_args = ClassificationArgs()\n",
        "    # Update model_args with hyperparameters applicable to simpletransformers... commenting out since I'm not sure if should instead pass in everything...\n",
        "    # simpletransformers_params = ['learning_rate', 'train_batch_size', 'num_train_epochs', 'max_seq_length']\n",
        "    # for param in simpletransformers_params:\n",
        "    #     if param in hyperparameters:\n",
        "    #         setattr(model_args, param, hyperparameters[param])\n",
        "    for param in hyperparameters:\n",
        "          setattr(model_args, param, hyperparameters[param])\n",
        "    for param in model_args_dict[model_id]:\n",
        "        setattr(model_args, param, model_args_dict[model_id][param])\n",
        "    for param in model_descriptions[model_id]:\n",
        "        setattr(model_args, param, model_descriptions[model_id][param])\n",
        "\n",
        "    model_args.manual_seed = random_state\n",
        "    model_args.overwrite_output_dir = True\n",
        "    model_args.reprocess_input_data = True\n",
        "    model_args.output_dir = f\"outputs/model_{model_id}\"\n",
        "\n",
        "    model = ClassificationModel(model_args_dict['transformer_type'], model_args_dict['transformer_name'], args=model_args, use_cuda=torch.cuda.is_available())\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "    # Add new special tokens\n",
        "    if special_tokens_dict:\n",
        "        tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        model.model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def encode_with_tokens(\n",
        "      tweet,\n",
        "      tokenizer,\n",
        "      feat_sep_token_id=None,\n",
        "      C_functions=None,\n",
        "      initial_token_id=None,\n",
        "      max_feature_tokens=None,\n",
        "      max_text_tokens=None\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Encode a text and additional features with separation tokens, providing more flexible\n",
        "      control over how much space features and text occupy.\n",
        "\n",
        "      Parameters:\n",
        "          tweet (str): The main text to encode.\n",
        "          tokenizer: The Hugging Face tokenizer to use for encoding.\n",
        "          feat_sep_token_id (int): Token ID for the feature separator token. If None,\n",
        "              defaults to the model's sep or eos token.\n",
        "          C_functions (list): List of feature functions that take the tweet as input and\n",
        "              return a feature value (string, number, or tuple).\n",
        "          initial_token_id (int): The initial token ID (e.g., CLS or BOS). If None,\n",
        "              defaults to tokenizer.cls_token_id or tokenizer.bos_token_id.\n",
        "          max_feature_tokens (int): Maximum total number of tokens allocated to features.\n",
        "              If None, no specific limit other than model_max_length is enforced.\n",
        "          max_text_tokens (int): Maximum number of tokens allocated to the main text.\n",
        "              If None, no specific limit other than model_max_length is enforced.\n",
        "\n",
        "      Returns:\n",
        "          list: The final encoded sequence of token IDs.\n",
        "      \"\"\"\n",
        "      # Determine initial token IDs and defaults\n",
        "      if initial_token_id is None:\n",
        "          initial_token_id = tokenizer.cls_token_id or tokenizer.bos_token_id\n",
        "\n",
        "      if feat_sep_token_id is None:\n",
        "          feat_sep_token_id = tokenizer.sep_token_id or tokenizer.eos_token_id\n",
        "\n",
        "      model_max_length = tokenizer.model_max_length\n",
        "\n",
        "      # Start sequence with initial token\n",
        "      encoded_sequence = [initial_token_id]\n",
        "\n",
        "      # Encode features\n",
        "      feature_tokens = []\n",
        "      if C_functions is not None:\n",
        "          for func in C_functions:\n",
        "              feature_tokens.append(feat_sep_token_id)\n",
        "              feature_value = func(tweet)\n",
        "              # Convert tuple values to space-separated string\n",
        "              if isinstance(feature_value, tuple):\n",
        "                  feature_value = ' '.join(map(str, feature_value))\n",
        "              else:\n",
        "                  feature_value = str(feature_value)\n",
        "\n",
        "              encoded_feature = tokenizer.encode(feature_value, add_special_tokens=False)\n",
        "              feature_tokens.extend(encoded_feature)\n",
        "\n",
        "          # If there's a max_feature_tokens limit, truncate\n",
        "          if max_feature_tokens is not None and len(feature_tokens) > max_feature_tokens:\n",
        "              feature_tokens = feature_tokens[:max_feature_tokens]\n",
        "\n",
        "      # Add the features to the main sequence (so far just initial token + features)\n",
        "      encoded_sequence.extend(feature_tokens)\n",
        "\n",
        "      # Encode the main tweet text\n",
        "      encoded_tweet = tokenizer.encode(tweet, add_special_tokens=True)\n",
        "\n",
        "      # If the initial token of encoded_tweet matches our initial_token_id (e.g., CLS), remove it\n",
        "      if encoded_tweet and encoded_tweet[0] == initial_token_id:\n",
        "          encoded_tweet = encoded_tweet[1:]\n",
        "\n",
        "      # Check how many tokens are still available before hitting model_max_length\n",
        "      # Reserve one token for the final separator\n",
        "      available_for_text = model_max_length - len(encoded_sequence) - 1\n",
        "      if max_text_tokens is not None:\n",
        "          # Further restrict available tokens for text if a max_text_tokens limit is set\n",
        "          available_for_text = min(available_for_text, max_text_tokens)\n",
        "\n",
        "      # Truncate text if necessary\n",
        "      if len(encoded_tweet) > available_for_text:\n",
        "          encoded_tweet = encoded_tweet[:available_for_text]\n",
        "\n",
        "      # Add the truncated tweet tokens\n",
        "      encoded_sequence.extend(encoded_tweet)\n",
        "\n",
        "      # Add final separator token at the end\n",
        "      encoded_sequence.append(feat_sep_token_id)\n",
        "\n",
        "      # In case we somehow exceed model_max_length after all operations, truncate\n",
        "      if len(encoded_sequence) > model_max_length:\n",
        "          encoded_sequence = encoded_sequence[:model_max_length]\n",
        "\n",
        "      return encoded_sequence\n",
        "\n",
        "\n",
        "def encode_with_tokens_based_on_model_id(tweet, tokenizer, model_id, model_args_dict, model_hyperparameters):\n",
        "  if 'separator_token_name' in model_args_dict[model_id]:\n",
        "    feat_sep_token_id = tokenizer.convert_tokens_to_ids(model_args_dict['separator_token_name'])\n",
        "  else:\n",
        "    feat_sep_token_id = None\n",
        "\n",
        "  if 'C_ids' in model_hyperparameters[model_id]:\n",
        "    C_functions = [C[i] for i in model_hyperparameters['C_ids']]\n",
        "  elif 'C_ids' in model_args_dict[model_id]:\n",
        "    C_functions = [C[i] for i in model_args_dict['C_ids']]\n",
        "  else:\n",
        "    C_functions = None\n",
        "\n",
        "  if 'max_feature_tokens' in model_hyperparameters[model_id]:\n",
        "    max_feature_tokens = model_hyperparameters[model_id]['max_feature_tokens']\n",
        "  elif 'max_feature_tokens' in model_args_dict[model_id]:\n",
        "    max_feature_tokens = model_args_dict[model_id]['max_feature_tokens']\n",
        "  else:\n",
        "    max_feature_tokens = None\n",
        "\n",
        "  if 'max_seq_length' in model_hyperparameters[model_id]:\n",
        "    max_seq_length = model_hyperparameters[model_id]['max_seq_length']\n",
        "  elif 'max_seq_length' in model_args_dict[model_id]:\n",
        "    max_seq_length = model_args_dict[model_id]['max_seq_length']\n",
        "  else:\n",
        "    max_seq_length = None\n",
        "\n",
        "  return encode_with_tokens(tweet,\n",
        "                            tokenizer,\n",
        "                            feat_sep_token_id = feat_sep_token_id,\n",
        "                            C_functions= C_functions,\n",
        "                            initial_token_id = None,\n",
        "                            max_feature_tokens=max_feature_tokens,\n",
        "                            max_seq_length =  max_seq_length)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "m7OjgILC7cRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "v5wR2e8cjn26"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COBCW8WVbpQ3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(X_train, X_val, model_id, model_hyperparameters, model_args_dict, wandb_project_name):\n",
        "    \"\"\"\n",
        "    Train a model based on the model_id and hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        model_id (int): The ID of the model to train.\n",
        "        hyperparameters (dict): Dictionary of hyperparameters for training.\n",
        "\n",
        "    Returns:\n",
        "        model: The trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up wandb run\n",
        "    current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    run_name = f\"Model_{model_id}_{current_time}\"\n",
        "    tags = [\n",
        "        f\"model_{model_id}\",\n",
        "        f\"date_{current_time.split('_')[0]}\",\n",
        "    ]\n",
        "    config_stuff = {**model_args_dict[model_id], **model_hyperparameters[model_id]}\n",
        "    # Add hyperparameters to tags\n",
        "    for param in config_stuff:\n",
        "        tags.append(f\"{param}_{config_stuff[param]}\")\n",
        "\n",
        "    # Initialize wandb run\n",
        "    run = wandb.init(\n",
        "        project=wandb_project_name,\n",
        "        name=run_name,\n",
        "        notes=model_descriptions[model_id],\n",
        "        tags=tags,\n",
        "        config=config_stuff\n",
        "    )\n",
        "\n",
        "    # TODO: call function above to get dict and another to initialize model\n",
        "    special_tokens_dict = create_special_tokens_dict(model_id, model_args_dict)\n",
        "    model, tokenizer = initialize_model_with_special_tokens(model_id, model_args_dict, model_hyperparameters, special_tokens_dict, model_descriptions)\n",
        "\n",
        "    # TODO: call function for train_data_prepared\n",
        "    train_data_prepared = encode_with_tokens_based_on_model_id(X_train, tokenizer, model_id, model_args_dict, model_hyperparameters)\n",
        "    val_data_prepared = encode_with_tokens_based_on_model_id(X_val, tokenizer, model_id, model_args_dict, model_hyperparameters)\n",
        "\n",
        "    # Log the model architecture\n",
        "    wandb.watch(model.model, log='all')\n",
        "\n",
        "    # Train the model\n",
        "    model.train_model(train_data_prepared)\n",
        "\n",
        "    # Evaluate the model on validation data\n",
        "    result, model_outputs, wrong_predictions = model.eval_model(val_data_prepared)\n",
        "\n",
        "    # Log evaluation metrics\n",
        "    wandb.log(result)\n",
        "\n",
        "    # Log an example input\n",
        "    wandb.log({'example_input': train_data_prepared['text'].iloc[0]})\n",
        "\n",
        "    # Log the model as an artifact with all necessary data for replication\n",
        "    artifact = wandb.Artifact(\n",
        "        name=f\"model_{model_id}_{run.id}\",\n",
        "        type='model',\n",
        "        description=model_descriptions[model_id],\n",
        "        metadata={\n",
        "            'model_id': model_id,\n",
        "            'model_hyperparams': model_hyperparameters,\n",
        "            'model_args': model_args_dict.to_dict(),\n",
        "            'random_state': random_state,\n",
        "        }\n",
        "    )\n",
        "    artifact.add_dir(model_args_dict.output_dir)\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    # Finish wandb run\n",
        "    run.finish()\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW2t9oNwjnmW",
        "outputId": "b0de615d-cd50-4254-cb3e-98460db36c8e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model 1: BERT*(tweet)\n",
            "Training Model 2: BERT*({tweet <SEP> C1 <SEP> C2 ...})\n",
            "Training Model 3: BERT*({tweet <FEAT> C1 <FEAT> C2 ...})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "1z4csoL7jri1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(X_test, model_id, model, tokenizer, model_hyperparameters, model_args_dict, wandb_project_name):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on test data.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        X_test (DataFrame): Test data.\n",
        "        hyperparameters (dict): Hyperparameters used during training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Prepare test data\n",
        "    test_data_prepared = encode_with_tokens_based_on_model_id(X_test, tokenizer, model_id, model_args_dict, model_hyperparameters)\n",
        "\n",
        "    # Evaluate the model\n",
        "    result, model_outputs, wrong_predictions = model.eval_model(test_data_prepared)\n",
        "\n",
        "    # Log evaluation metrics to wandb\n",
        "    wandb.log(result)\n",
        "\n",
        "    true_labels = test_data_prepared['label']\n",
        "    predictions = np.argmax(model_outputs, axis=1)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Log confusion matrix\n",
        "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvKz6fYrjqXG",
        "outputId": "35c71961-8f8b-4263-9b7e-c0e63741994b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Model 1\n",
            "Evaluating Model 2\n",
            "Evaluating Model 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sweep config and training function"
      ],
      "metadata": {
        "id": "StNI2g5xtinG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter space\n",
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'metric': {\n",
        "        'name': 'eval_loss',\n",
        "        'goal': 'minimize'\n",
        "    },\n",
        "    # TODO\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT_NAME)\n",
        "\n",
        "def sweep_train():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    # Get hyperparameters from wandb.config. I only want to sweep through hyper\n",
        "    # TODO\n",
        "    # Train the model\n",
        "    # TODO\n",
        "    # Evaluate the model on test data\n",
        "    # TODO\n",
        "\n",
        "\n",
        "# Start the sweep with wandb.agent\n",
        "#TODO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xG_No0wSthms",
        "outputId": "24b3c837-493f-47c7-da54-1681bb3d382b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-07bb5073eb7a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msweep_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msweep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWANDB_PROJECT_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msweep_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hOi4_U3etokY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion & next steps\n",
        "# Conclusion\n",
        "\n",
        "- We have set up the notebook structure for training and evaluating multiple models.\n",
        "- The data loading and feature functions are integrated as per your requirements.\n",
        "- We have placeholders for training and evaluation functions where wandb integration will be added.\n",
        "- Next steps include implementing the TODO sections and ensuring comprehensive wandb logging.\n",
        "\n",
        "# Next Steps\n",
        "\n",
        "- Implement the `prepare_data` function to handle data preparation for each model.\n",
        "- Complete the `train_model` function with wandb integration, including logging hyperparameters, metrics, and artifacts.\n",
        "- Implement the `evaluate_model` function to evaluate models on validation and test data.\n",
        "- Fine-tune hyperparameters using wandb sweeps if desired.\n",
        "- Analyze the results and prepare reports using wandb's reporting tools.\n"
      ],
      "metadata": {
        "id": "49lpKdT7ju9t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FidWYYCUjtc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}